{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8122bd6-0f61-4865-b586-fbb100ace5db",
   "metadata": {},
   "source": [
    "# Matrix Multiplication: The Engine of Machine Learning\n",
    "\n",
    "Matrix multiplication powers many key algorithms in ML. Below are examples from neural networks, PCA, and linear regression.\n",
    "\n",
    "1. Neural Networks\n",
    "\n",
    "## 1. Neural Networks (signal propagation)\n",
    "\n",
    "When you feed data into a neural network, matrix multiplication moves signals forward between layers.\n",
    "\n",
    "Example: Suppose we have 3 inputs (features) going into a hidden layer with 2 neurons.\n",
    "\n",
    "\\[\n",
    "x =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{bmatrix}, \\quad\n",
    "W =\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.5 & 0.1 \\\\\n",
    "0.4 & 0.3 & 0.7\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "Multiply: \\(z = W \\cdot x\\)\n",
    "\n",
    "\\[\n",
    "z =\n",
    "\\begin{bmatrix}\n",
    "1.5 \\\\\n",
    "3.1\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2854882b-36d2-4800-be80-d3eed7f3be74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5],\n",
       "       [3.1]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1], [2], [3]])\n",
    "W = np.array([[0.2, 0.5, 0.1],\n",
    "              [0.4, 0.3, 0.7]])\n",
    "\n",
    "z = W @ x\n",
    "z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3155933-2c23-488d-a784-8b73fb3fa5f1",
   "metadata": {},
   "source": [
    "2. Principal Component Analysis (PCA)\n",
    "## 2. Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is used for dimensionality reduction. The key is computing the covariance matrix, which itself requires matrix multiplication.\n",
    "\n",
    "\\[\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "2 & 3 \\\\\n",
    "3 & 4 \\\\\n",
    "4 & 5\n",
    "\\end{bmatrix}, \\quad\n",
    "C = \\frac{1}{n-1} X^\\top X\n",
    "\\]\n",
    "\n",
    "After multiplication:\n",
    "\n",
    "\\[\n",
    "X^\\top X =\n",
    "\\begin{bmatrix}\n",
    "29 & 38 \\\\\n",
    "38 & 50\n",
    "\\end{bmatrix}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4f8ff-2289-45a4-aab5-08f444f468ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[2, 3],\n",
    "              [3, 4],\n",
    "              [4, 5]])\n",
    "\n",
    "C = (X.T @ X) / (X.shape[0] - 1)\n",
    "C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c482cbb6-7943-4dca-ba79-3e4048a5fe0f",
   "metadata": {},
   "source": [
    "3. Linear Regression (Normal Equation)\n",
    "## 3. Linear Regression (Normal Equation)\n",
    "\n",
    "In linear regression, coefficients are solved using:\n",
    "\n",
    "\\[\n",
    "\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n",
    "\\]\n",
    "\n",
    "Example with 2 data points:\n",
    "\n",
    "\\[\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}, \\quad\n",
    "y =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2eab4-d71c-4392-8d9b-13ab357300e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1],\n",
    "              [1, 2]])\n",
    "y = np.array([[1],\n",
    "              [2]])\n",
    "\n",
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949229c0-8fbd-41c6-910a-2d0b5aba0757",
   "metadata": {},
   "source": [
    "## Why Matrix Multiplication is Central\n",
    "\n",
    "- In neural networks, itâ€™s how weights and activations combine.  \n",
    "- In PCA, it builds covariance and finds directions of maximum variance.  \n",
    "- In regression, it solves for optimal weights directly.  \n",
    "\n",
    "Every time ML models learn, transform, or make predictions, matrix multiplication is working behind the scenes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
